# create kind cluster
kind create cluster --config ~/git/ldk/kind-cluster/klc.yaml

# to delete cluster
kind delete cluster --name=klc

# shell into the control plane
docker exec -it klc-control-plane bash

# stop kind containers
docker stop $(docker ps -q --filter "name=klc")

# to start kind containers
docker start $(docker ps -aq --filter "name=klc")

# run these on the kind cluster
kubectl run nginx --image=nginx

kubectl get pods

kubectl logs pod/nginx

# get more pod info
kubectl get pods -o wide

# output
NAME    READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          115s   10.244.1.2   klc-worker2   <none>           <none>

# install ping on the control plane node to be able to ping pod IPs
apt update
apt install -y iputils-ping

# curl nginx in the pod
root@klc-control-plane:/# curl 10.244.1.2

# run port forward on the host machine to expose nginx in the pod to the host
# access http://127.0.0.1:8080/ in the browser
sysuser@ubuntudev:~/git/docker$ kubectl port-forward pod/nginx 8080:80

# curl can also be used to access nginx on the pod
sysuser@ubuntudev:~/git/docker$ curl localhost:8080

# run this on the control plane. this will start a curl pod which will curl the nginx pod and exit
kubectl run -it --rm curlpod --image=curlimages/curl --restart=Never -- 10.244.1.2

# run ubuntu pod in the background
kubectl run ubuntupod --image=ubuntu sleep infinity 

# shell into pod: ubuntu pod which is currently sleeping
kubectl exec -it ubuntupod -- bash

# run the following inside ubuntupod
ps -ef

# output
root@ubuntupod:/# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 23:55 ?        00:00:00 sleep infinity
root          12       0  0 23:57 pts/0    00:00:00 bash
root          21      12  0 23:58 pts/0    00:00:00 ps -ef

# run from control plane pod # 

# delete both pods 
kubectl delete pod/nginx pod/ubuntupod --now

# we can use a dry run to output pod config  as a yaml and pipe it to a file using tee
kubectl run nginxpod --image=nginx --dry-run=client -o yaml | tee nginxpod.yaml

# use the create option to create the pod by specifying the yaml file
root@klc-control-plane:/# kubectl create -f nginxpod.yaml

# create ubuntu pod
root@klc-control-plane:/# kubectl run ubuntupod --image=ubuntu --dry-run=client -o yaml sleep infinity | tee ubuntupod.yaml

# create the pod. apply command can be used
root@klc-control-plane:/# kubectl apply -f ubuntupod.yaml 

# get the pods
root@klc-control-plane:/# kubectl get pods -o wide

# output
NAME        READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
nginxpod    1/1     Running   0          4m51s   10.244.2.2   klc-worker2   <none>           <none>
ubuntupod   1/1     Running   0          22s     10.244.1.2   klc-worker    <none>           <none>

# we want to combine both the yaml files into one file called combined.yaml
# grep to make sure both the yaml files exist
root@klc-control-plane:/# ls -l | grep yaml

# output
root@klc-control-plane:/# ls -l | grep yaml
-rw-r--r--   1 root root  215 Jan 31 00:32 nginxpod.yaml
-rw-r--r--   1 root root  256 Jan 31 00:38 ubuntupod.yaml

# run the following
root@klc-control-plane:/# { cat nginxpod.yaml ; echo "---"; cat ubuntupod.yaml; } | tee combined.yaml

# run the grep command
root@klc-control-plane:/# ls -l | grep yaml

# output
root@klc-control-plane:/# ls -l | grep yaml
-rw-r--r--   1 root root  475 Jan 31 00:44 combined.yaml
-rw-r--r--   1 root root  215 Jan 31 00:32 nginxpod.yaml
-rw-r--r--   1 root root  256 Jan 31 00:38 ubuntupod.yaml

# apply the combined yaml
root@klc-control-plane:/# kubectl apply -f combined.yaml 

# delete the pods
root@klc-control-plane:/# kubectl delete -f combined.yaml --now


# copy mypod.yaml from host to controlplane container
sysuser@ubuntudev:~/git/docker/kind-cluster$ docker cp /home/sysuser/git/docker/kcna/kubernetes/mypod.yaml klc-control-plane:/mypod.yaml

# apply mypod.yaml to create the pod with two containers
root@klc-control-plane:/# kubectl apply -f mypod.yaml -all-namespaces

# this would show 2/2 under ready column because there are two containers running in the same pod
root@klc-control-plane:/# kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
mypod   2/2     Running   0          3m36s   10.244.1.4   klc-worker   <none>           <none>


# get all the information about mypod
root@klc-control-plane:/# kubectl describe pod/mypod

# curl the shared ip address of mypod
root@klc-control-plane:/# kubectl run -it --rm curlpod --image=curlimages/curl --restart=Never -- 10.244.2.2

# output
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
pod "curlpod" deleted from default namespace

# to make sure the sidecar is doing what its supposed to, we can get the logs from the container in the pod
root@klc-control-plane:/# kubectl logs mypod -c ubuntu-sidecar

# create a crash file so that the side car would crash
root@klc-control-plane:/# kubectl exec -it mypod -c ubuntu-sidecar -- touch /tmp/crash

# get pod information
root@klc-control-plane:/# kubectl get pods -o wide
NAME    READY   STATUS   RESTARTS      AGE   IP           NODE         NOMINATED NODE   READINESS GATES
mypod   1/2     Error    2 (80s ago)   22m   10.244.2.5   klc-worker   <none>           <none>


root@klc-control-plane:/# kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS      AGE   IP           NODE         NOMINATED NODE   READINESS GATES
mypod   2/2     Running   3 (43s ago)   23m   10.244.2.5   klc-worker   <none>           <none>

### NAMESPACES ###

# get shortnames of all the resources
kubectl api-resources | more

# get all namesapces
kubectl get ns

# view the current configuration used by kubectl
sysuser@ubuntudev:~$ kubectl config view

# create a pod in a non default namespace by first creating the namespace and then creating a pod in the namespace
sysuser@ubuntudev:~$ kubectl create namespace myns && kubectl -n myns run nginx --image=nginx

# use this query to get pods from all namespaces
sysuser@ubuntudev:~$ kubectl get pods --all-namespaces
OR
sysuser@ubuntudev:~$ kubectl get pods -A

# use this query to get pods from a specific non default namespace
sysuser@ubuntudev:~$ kubectl -n myns get pods -o wide

# to change the default namespace when querying
sysuser@ubuntudev:~$ kubectl config set-context --current --namespace=myns

### Deployments and ReplicaSetS ###

# create a deployment with nginx image
root@klc-control-plane:/# kubectl create deployment nginxdeployment --image=nginx --dry-run=client -o yaml | tee nginxdeployment.yaml

# copy the deployment from the control plane to host
sysuser@ubuntudev:~/git/ldk/kind-cluster$ docker cp klc-control-plane:/nginxdeployment.yaml /home/sysuser/git/ldk/kcna/kubernetes/nginxdeployment.yaml

# apply the deployment
root@klc-control-plane:/# kubectl apply -f nginxdeployment.yaml

# get deployment
root@klc-control-plane:/# kubectl get deployment

# OUTPUT
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
nginxdeployment   1/1     1            1           13s

# deployments automatically create ReplicaSetS
root@klc-control-plane:/# kubectl get replicaset -o wide

# OUTPUT
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
nginxdeployment-6fd49c696   1         1         1       47s   nginx        nginx    app=nginxdeployment,pod-template-hash=6fd49c696

# get pods pods
root@klc-control-plane:/# kubectl get pods -o wide

# OUTPUT
NAME                              READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES
nginxdeployment-6fd49c696-k946p   1/1     Running   0          108s   10.244.2.3   klc-worker2   <none>           <none>

# get rollout history for deployments
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# OUTPUT
REVISION  CHANGE-CAUSE
1         <none>

# annotate deployment history
root@klc-control-plane:/# kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="initial deployment"

# get rollout deployment history
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# OUTPUT
deployment.apps/nginxdeployment
REVISION  CHANGE-CAUSE
1         initial deployment

# increase the number of replicas and watch the progress
root@klc-control-plane:/# kubectl scale deployment/nginxdeployment --replicas 4; watch kubectl get pods -o wide

# output
NAME                              READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
nginxdeployment-6fd49c696-b57pt   1/1     Running   0          2m41s   10.244.1.5   klc-worker   <none>           <none>
nginxdeployment-6fd49c696-l6hh2   1/1     Running   0          2m41s   10.244.1.4   klc-worker   <none>           <none>
nginxdeployment-6fd49c696-ntl2w   1/1     Running   0          2m41s   10.244.1.6   klc-worker   <none>           <none>
nginxdeployment-6fd49c696-sjvh2   1/1     Running   0          2m59s   10.244.1.2   klc-worker   <none>           <none>

root@klc-control-plane:/# kubectl get deployment -o wide

# output
NAME              READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
nginxdeployment   4/4     4            4           12m   nginx        nginx    app=nginxdeployment

# edit the deployment to change the number of replicas to 6
root@klc-control-plane:/# nano nginxdeployment.yaml

# apply changes
root@klc-control-plane:/# kubectl apply -f nginxdeployment.yaml

# output
deployment.apps/nginxdeployment configured

# get deployment
root@klc-control-plane:/# kubectl get deployment -o wide

# output
NAME              READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
nginxdeployment   6/6     6            6           14m   nginx        nginx    app=nginxdeployment

# rollout history is not affected at this point
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# output
deployment.apps/nginxdeployment
REVISION  CHANGE-CAUSE
1         initial deployment

# get the applied deployment for nginxdeployment
root@klc-control-plane:/# kubectl get deployment/nginxdeployment -o yaml | tee nginxdeployment-applied.yaml

# copy the file to localhost
sysuser@ubuntudev:~/git/ldk$ docker cp klc-control-plane:/nginxdeployment-applied.yaml /home/sysuser/git/ldk/kcna/kubernetes/nginxdeployment-applied.yaml

# Change the version of nginx in the original deployment
root@klc-control-plane:/# nano nginxdeployment.yaml

# copy the yaml to host
sysuser@ubuntudev:~/git/ldk$ docker cp klc-control-plane:/nginxdeployment.yaml /home/sysuser/git/ldk/kcna/kubernetes/nginxdeployment-stable.yaml

# apply the changes and watch the rollout
root@klc-control-plane:/# kubectl apply -f nginxdeployment.yaml && kubectl rollout status deployment/nginxdeployment

# OUTPUT
deployment.apps/nginxdeployment configured
Waiting for deployment "nginxdeployment" rollout to finish: 0 out of 20 new replicas have been updated...
Waiting for deployment "nginxdeployment" rollout to finish: 5 out of 20 new replicas have been updated...
Waiting for deployment "nginxdeployment" rollout to finish: 10 out of 20 new replicas have been updated...
.
.
Waiting for deployment "nginxdeployment" rollout to finish: 11 out of 20 new replicas have been updated...
.
Waiting for deployment "nginxdeployment" rollout to finish: 12 out of 20 new replicas have been updated...
.
.
.
Waiting for deployment "nginxdeployment" rollout to finish: 18 out of 20 new replicas have been updated...
Waiting for deployment "nginxdeployment" rollout to finish: 19 out of 20 new replicas have been updated...

Waiting for deployment "nginxdeployment" rollout to finish: 5 old replicas are pending termination...
.
Waiting for deployment "nginxdeployment" rollout to finish: 4 old replicas are pending termination...
.
.
Waiting for deployment "nginxdeployment" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "nginxdeployment" rollout to finish: 1 old replicas are pending termination...
.
Waiting for deployment "nginxdeployment" rollout to finish: 15 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 15 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 16 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 17 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 17 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 18 of 20 updated replicas are available...
Waiting for deployment "nginxdeployment" rollout to finish: 19 of 20 updated replicas are available...
deployment "nginxdeployment" successfully rolled out

# annotatethe history
root@klc-control-plane:/# kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="change image to nginx stable"

# get the history
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# OUTPUT
REVISION  CHANGE-CAUSE
3         initial deployment
4         change image to nginx stable

# imperative change to deployment
root@klc-control-plane:/# kubectl set image deployment/nginxdeployment nginx=nginx:alpine && kubectl rollout status deployment/nginxdeployment

# incorrect tag to watch for failures
root@klc-control-plane:/# kubectl set image deployment/nginxdeployment nginx=nginx:xyzzz

# annotate the failure
root@klc-control-plane:/# kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="bad version"

# describe the deployment
root@klc-control-plane:/# kubectl describe deployment/nginxdeployment

# OUTPUT
Name:                   nginxdeployment
Namespace:              default
CreationTimestamp:      Sat, 07 Feb 2026 20:02:09 +0000
Labels:                 app=nginxdeployment
Annotations:            deployment.kubernetes.io/revision: 9
                        kubernetes.io/change-cause: bad version
Selector:               app=nginxdeployment
Replicas:               8 desired | 4 updated | 10 total | 6 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginxdeployment
  Containers:
   nginx:
    Image:         nginx:xyzzz
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  nginxdeployment-6fd49c696 (0/0 replicas created), nginxdeployment-7b48fbd75 (6/6 replicas created), nginxdeployment-64d46c7cf9 (0/0 replicas created)
NewReplicaSet:   nginxdeployment-595c6ddcbf (4/4 replicas created)
Events:
  Type    Reason             Age                   From                   Message
  ----    ------             ----                  ----                   -------
  Normal  ScalingReplicaSet  19m                   deployment-controller  Scaled up replica set nginxdeployment-6fd49c696 from 0 to 6
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled up replica set nginxdeployment-6fd49c696 from 6 to 8
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled up replica set nginxdeployment-7b48fbd75 from 0 to 2
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled down replica set nginxdeployment-6fd49c696 from 8 to 6
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled up replica set nginxdeployment-7b48fbd75 from 2 to 4
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled down replica set nginxdeployment-6fd49c696 from 6 to 5
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled up replica set nginxdeployment-7b48fbd75 from 4 to 5
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled down replica set nginxdeployment-6fd49c696 from 5 to 4
  Normal  ScalingReplicaSet  17m                   deployment-controller  Scaled up replica set nginxdeployment-7b48fbd75 from 5 to 6
  Normal  ScalingReplicaSet  4m11s (x54 over 17m)  deployment-controller  (combined from similar events): Scaled down replica set nginxdeployment-595c6ddcbf from 4 to 0

# rollback the change to the deployment
root@klc-control-plane:/# kubectl rollout undo deployment/nginxdeployment

# get history
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# OUTPUT
REVISION  CHANGE-CAUSE
4         alpine version
5         alpine version
9         bad version
10        alpine version

# rollback to a specific version
root@klc-control-plane:/# kubectl rollout undo deployment/nginxdeployment --to-revision=4 && kubectl rollout status deployment/nginxdeployment

##### combine operations ####

# create different yaml files for each type of image including bad image and apply each and annotate

# command to apply initial deployment with no image tag
root@klc-control-plane:/# kubectl apply -f nginxdeployment.yaml && kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="initial deployment" && kubectl rollout status deployment/nginxdeployment

# command to apply stable deployment
root@klc-control-plane:/# kubectl apply -f nginxdeployment-stable.yaml && kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="stable deployment" && kubectl rollout status deployment/nginxdeployment

# command to apply alpine deployment
root@klc-control-plane:/# kubectl apply -f nginxdeployment-alpine.yaml && kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="alpine deployment" && kubectl rollout status deployment/nginxdeployment

# bad deployment
root@klc-control-plane:/# kubectl apply -f nginxdeployment-bad.yaml && kubectl annotate deployment/nginxdeployment kubernetes.io/change-cause="bad deployment"

# history
root@klc-control-plane:/# kubectl rollout history deployment/nginxdeployment

# OUTPUT
deployment.apps/nginxdeployment
REVISION  CHANGE-CAUSE
1         initial deployment
2         stable deployment
3         alpine deployment
4         bad deployment

# rollback
root@klc-control-plane:/# kubectl rollout undo deployment/nginxdeployment

# OUTPUT
REVISION  CHANGE-CAUSE
1         initial deployment
2         stable deployment
4         bad deployment
5         alpine deployment

### ClusterIP ###

# Running the tests on host

# get a yaml for an nginx deployment with 3 replicas
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl create deployment nginxdep --image=spurin/nginx-debug --port=80 --replicas=3 -o yaml --dry-run=client | tee nginxdep.yaml

# apply the deployment
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl apply -f nginxdep.yaml

# get a yaml to expose the deployment. This creates a service. If no type is mentioned, it will be a clusterIP
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl expose deployment/nginxdep --name=nginxsvc --dry-run=client -o yaml | tee nginxsvc.yaml

# apply the service
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl apply -f nginxsvc.yaml

# output
service/nginxsvc created

# get the service
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl get svc

# output
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   18h
nginxsvc     ClusterIP   10.96.119.87   <none>        80/TCP    28s

# get the endpoints
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl get endpointSlice

# output
NAME             ADDRESSTYPE   PORTS   ENDPOINTS                          AGE
kubernetes       IPv4          6443    172.18.0.3                         18h
nginxsvc-4mvl2   IPv4          80      10.244.2.6,10.244.1.6,10.244.2.7   3m17s

# get service description
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl describe service/nginxsvc

# output
Name:                     nginxsvc
Namespace:                default
Labels:                   app=nginxdep
Annotations:              <none>
Selector:                 app=nginxdep
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.119.87
IPs:                      10.96.119.87
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
Endpoints:                10.244.2.6:80,10.244.1.6:80,10.244.2.7:80
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>

# run curl to the endpoints from the control plane. port forwarding is not enabled yet
root@klc-control-plane:/# curl 10.244.2.6

# output
<!DOCTYPE html>
<body>
<p><em>Hostname: nginxdep-588fb5967c-tkbnt</em></p>
<p><em>IP Address: 10.244.2.6:80</em></p>
<p><em>URL: /</em></p>
<p><em>Request Method: GET</em></p>
<p><em>Request ID: 362911430815223af049018dfa951655</em></p>
</body>
</html>

# port forward to 8080 on the host so that we can curl the endpoints from the host using localhost:8080
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl port-forward service/nginxsvc 8080:80

# create curlpod.yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl run curlpod --dry-run=client -o yaml --image=curlimages/curl --restart=Never --command -- sh -c "sleep infinity" | tee curlpod.yaml

# apply the pod yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl apply -f curlpod.yaml

# shell into pod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/clusterip$ kubectl exec -it curlpod -- sh

# run the following command from the curlpod. this will run curl every 2 seconds
# <servicename>.<namespace>.svc.cluster.local
~ $ watch curl nginxsvc.default.svc.cluster.local

# output. most importantly, the hostname is going to change between different pods (there are 3 replicas)
Every 2.0s: curl nginxsvc.default.svc.cluster.local                                                                                                                               2026-02-08 15:30:28

<!DOCTYPE html>
<body>
<p><em>Hostname: nginxdep-588fb5967c-d257p</em></p>
<p><em>IP Address: 10.244.2.7:80</em></p>
<p><em>URL: /</em></p>
<p><em>Request Method: GET</em></p>
<p><em>Request ID: 7fecb354e76997d2fefd938368254699</em></p>
</body>
</html>

# get the dns names and the dns server
~ $ cat /etc/resolv.conf

# output
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

# this is valid
~ $ curl nginxsvc

##### NodePort ####

# delete existing clusterip service if running
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/nodeport$ kubectl delete svc/nginxsvc --now

# create a nodeport service yaml from deployment/nginxdep
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/nodeport$ kubectl expose deployment/nginxdep --name=nginxnp --dry-run=client -o yaml --type=NodePort | tee nginxnp.yaml

# apply the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/nodeport$ kubectl apply -f nginxnp.yaml

# get the services
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/nodeport$ kubectl get svc -o wide

# output
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        19h   <none>
nginxnp      NodePort    10.96.204.212   <none>        80:30732/TCP   9s    app=nginxdep

# get the nodes
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/nodeport$ kubectl get nodes -o wide

NAME                STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME
klc-control-plane   Ready    control-plane   19h   v1.35.0   172.18.0.3    <none>        Debian GNU/Linux 12 (bookworm)   6.12.67-linuxkit   containerd://2.2.0
klc-worker          Ready    <none>          19h   v1.35.0   172.18.0.4    <none>        Debian GNU/Linux 12 (bookworm)   6.12.67-linuxkit   containerd://2.2.0
klc-worker2         Ready    <none>          19h   v1.35.0   172.18.0.2    <none>        Debian GNU/Linux 12 (bookworm)   6.12.67-linuxkit   containerd://2.2.0

# curl from controlplane. this should work
root@klc-control-plane:/# curl 172.18.0.2:30732

##### load balancer #####

# run the command to create load balancer service from the deployment
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ kubectl expose deployment/nginxdep --dry-run=client -o yaml --port 8080 --target-port 80 --name=nginxlb --type=LoadBalancer | tee nginxlb.yaml

# apply the load balancer service
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ kubectl apply -f nginxlb.yaml

# get svc
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ kubectl get svc

# output
NAME         TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP      10.96.0.1     <none>        443/TCP          36m
nginxlb      LoadBalancer   10.96.51.75   172.18.0.5    8080:31377/TCP   8m18s

# There are two ways to hit the application deployed behind the load balancer
# from the control plane inside the cluster
# use internal-ip of the node obtained by kubectl get nodes -o wide and the load balancer port (31377)
# syntax watch -differences "curl <node_ip>:<node_port> 2>/dev/null"
root@klc-control-plane:/# watch -differences "curl 172.18.0.3:31377 2>/dev/null"

# from local host by obtaining the mapped docker port for the envoy proxy
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ docker ps | grep kindccm

# output
f9719c91209f   envoyproxy/envoy:v1.33.2        "/docker-entrypoint.â€¦"   5 minutes ago    Up 5 minutes    0.0.0.0:33957->8080/tcp, [::]:33957->8080/tcp, 0.0.0.0:39601->10000/tcp, [::]:39601->10000/tcp   kindccm-6d258beb92a7

# the port mapped to 8080 (service port), which is 33957 is what we will use
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ watch -differences "curl localhost:33957 2>/dev/null"

# the external ip can be used from within the cluster along with service port
root@klc-control-plane:/# curl 172.18.0.5:8080

# from within the cluster, each node in the cluster including control plane has the mapped port open on it (31377) and this will
root@klc-control-plane:/# curl localhost:31377

## load balancer and nodeport service do the same thing locally. load balancer is not needed for local testing. nodeport is better for this purpose. cloud-provider-kind is a good learning exercise and makes life easy when combined with docker. ##

# scale down the replicas and watch the curl command only hit one pod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/loadbalancer$ kubectl scale deployment/nginxdep --replicas=1

######################
#### ExternalName ####
######################

# create two deployments
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl create deployment nginxreddep --port 80 --image=spurin/nginx-red --dry-run=client -o yaml | tee nginxreddep.yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl create deployment nginxbluedep --port 80 --image=spurin/nginx-blue --dry-run=client -o yaml | tee nginxbluedep.yaml

# create two clusterip services
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl expose deployment/nginxreddep --type=ClusterIP --name=nginxredcip --dry-run=client -o yaml | tee nginxredcip.yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl expose deployment/nginxbluedep --type=ClusterIP --name=nginxbluecip --dry-run=client -o yaml | tee nginxbluecip.yaml

# apply all the deployment files
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl apply -f nginxreddep.yaml -f nginxbluedep.yaml -f nginxredcip.yaml -f nginxbluecip.yaml

# create two external service name yaml files
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl create service externalname nginxredextname --external-name nginxredcip.default.svc.cluster.local --dry-run=client -o yaml | tee nginxredextname.yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl create service externalname nginxblueextname --external-name nginxbluecip.default.svc.cluster.local --dry-run=client -o yaml | tee nginxblueextname.yaml

# apply the external services
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl apply -f nginxredextname.yaml -f nginxblueextname.yaml

# create a curlpod to demo external name service
kubectl run curlpod --dry-run=client -o yaml --image=curlimages/curl --restart=Never --command -- sh -c "sleep infinity" | tee curlpod.yaml

# create external name service
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl create service externalname myextnamesvc --external-name nginxredcip.default.svc.cluster.local --dry-run=client -o yaml | tee myextnamesvc.yaml

# shell into the curlpod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl exec -it curlpod -- sh

# curl the external name service created prior
~ $ curl nginxredextname
~ $ curl nginxblueextname

##########################
### HEADLESS ClusterIP ###
##########################

# headless service is a clusterip service without a cluster ip attached to it. this serves as a dns where a request would be hitting the pod directly instead of a service which routes to a pod
# create a headless deployment
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/headless$ kubectl create deployment nginxheadlessdep --image=spurin/nginx-debug --replicas=3 --port 80 --dry-run=client -o yaml | tee nginxheadlessdep.yaml

# create a cluster ip from the deployment and edit the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/headless$ kubectl expose deployment/nginxheadlessdep --type=ClusterIP --name=nginxheadlesscip --dry-run=client -o yaml | tee nginxheadlesscip.yaml

# edit the yaml and set clusterIP: None under 'spec' section
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/headless$ nano nginxheadlesscip.yaml

# apply the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/headless$ kubectl apply -f nginxheadlesscip.yaml

# get services and not that nginxheadlesscip does not contain a cluster-ip
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/headless$ kubectl get svc
NAME               TYPE           CLUSTER-IP      EXTERNAL-IP                              PORT(S)   AGE
kubernetes         ClusterIP      10.96.0.1       <none>                                   443/TCP   4h3m
nginxbluecip       ClusterIP      10.96.199.173   <none>                                   80/TCP    58m
nginxblueextname   ExternalName   <none>          nginxbluecip.default.svc.cluster.local   <none>    36m
nginxheadlesscip   ClusterIP      None            <none>                                   80/TCP    6s
nginxredcip        ClusterIP      10.96.208.199   <none>                                   80/TCP    58m
nginxredextname    ExternalName   <none>          nginxredcip.default.svc.cluster.local    <none>    36m

# shell into the curlpod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/externalname$ kubectl exec -it curlpod -- sh


# curl headless cluster ip by service name to see the pod ips in the IP Address field.
/ $ watch curl nginxheadlesscip

# OUTPUT
Every 2.0s: curl nginxheadlesscip                                                                                                         2026-02-13 20:11:08

<!DOCTYPE html>
<body>
<p><em>Hostname: nginxheadlessdep-7586fd8466-7skzc</em></p>
<p><em>IP Address: 10.244.1.11:80</em></p>
<p><em>URL: /</em></p>
<p><em>Request Method: GET</em></p>
<p><em>Request ID: aa0ec151f4304f7503d8e0d61f2319dc</em></p>
</body>
</html>

##############
#### JOBS ####
##############

# create pcilalcjob
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl create job calcpijob --dry-run=client -o yaml --image=perl:5.34.0 -- "perl" "-Mbignum=bpi" "-wle" "print bpi(2000)" | tee calcpijob.yaml

# get the pod name running the job
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl get pods -o wide

# get logs for that pod. it will be pi calculated to 2000 decimal points.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl logs calcpijob-xfk4j

# get applied yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl get job/calcpijob -o yaml | tee calcpijob-applied.yaml

# edit the job yaml and set completion to 20 and parallelism to 5.
# delete the job
sysuser@ubuntudev:~$ kubectl delete job/calcpijob

# apply the job and watch the pods
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl apply -f calcpijob.yaml

# create a job that runs an ubuntu image to sleep for 20 seconds and exists. set completions to 20 and parallelism to 5
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl create job sleep20 --image=ubuntu --dry-run=client -o yaml -- sleep 20 | tee sleep20.yaml

# apply the job
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs$ kubectl apply -f sleep20.yaml

# watch the pods
kubectl get pods -o wide

# output. there will be 20 completed pods at the end and during the job there will be 5 running
Every 5.0s: kubectl get pods -o wide                                                                                                      ubuntudev: Sat Feb 14 08:02:53 2026

NAME            READY   STATUS      RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
sleep20-2r8bx   0/1     Completed   0          40s   10.244.2.44   klc-worker2   <none>           <none>
sleep20-4dtjc   1/1     Running     0          16s   10.244.1.28   klc-worker    <none>           <none>
sleep20-69nq8   0/1     Completed   0          88s   10.244.2.37   klc-worker2   <none>           <none>
sleep20-6d4ng   1/1     Running     0          16s   10.244.2.45   klc-worker2   <none>           <none>
sleep20-6wb9h   0/1     Completed   0          64s   10.244.2.40   klc-worker2   <none>           <none>
sleep20-bzdgv   0/1     Completed   0          65s   10.244.1.24   klc-worker    <none>           <none>
sleep20-cfdnt   0/1     Completed   0          65s   10.244.2.39   klc-worker2   <none>           <none>
sleep20-cm64p   0/1     Completed   0          64s   10.244.2.41   klc-worker2   <none>           <none>
sleep20-czb8j   1/1     Running     0          16s   10.244.1.29   klc-worker    <none>           <none>
sleep20-f7b47   0/1     Completed   0          65s   10.244.1.25   klc-worker    <none>           <none>
sleep20-fg8rn   1/1     Running     0          15s   10.244.2.47   klc-worker2   <none>           <none>
sleep20-jmv7g   0/1     Completed   0          40s   10.244.2.43   klc-worker2   <none>           <none>
sleep20-jrlpp   0/1     Completed   0          41s   10.244.2.42   klc-worker2   <none>           <none>
sleep20-k84qv   0/1     Completed   0          41s   10.244.1.27   klc-worker    <none>           <none>
sleep20-mv9lj   0/1     Completed   0          88s   10.244.1.22   klc-worker    <none>           <none>
sleep20-qgmtf   0/1     Completed   0          88s   10.244.1.23   klc-worker    <none>           <none>
sleep20-rlc2q   0/1     Completed   0          41s   10.244.1.26   klc-worker    <none>           <none>
sleep20-sczds   1/1     Running     0          15s   10.244.2.46   klc-worker2   <none>           <none>
sleep20-tcs46   0/1     Completed   0          88s   10.244.2.36   klc-worker2   <none>           <none>
sleep20-xqbn4   0/1     Completed   0          88s   10.244.2.38   klc-worker2   <none>           <none>

#################
#### cronjob ####
#################

# create a cronjob that runs every minute.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs/cronjob$ kubectl create cronjob sleep5 --image=ubuntu --schedule="* * * * *" --dry-run=client -o yaml -- sleep 5 | tee sleep5.yaml


# edit the file, set completions to 5 and parallelism to 3
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs/cronjob$ kubectl apply -f sleep5.yaml

# watch the pods
kubectl get pods -o wide

# output
Every 5.0s: kubectl get pods -o wide | grep sleep5                                                                                        ubuntudev: Sat Feb 14 08:20:04 2026

sleep5-29517977-4z9nz   0/1     Completed   0          2m56s   10.244.1.39   klc-worker    <none>           <none>
sleep5-29517977-bzfkk   0/1     Completed   0          3m5s    10.244.2.61   klc-worker2   <none>           <none>
sleep5-29517977-l4g7k   0/1     Completed   0          2m56s   10.244.2.62   klc-worker2   <none>           <none>
sleep5-29517977-qx64c   0/1     Completed   0          3m5s    10.244.1.38   klc-worker    <none>           <none>
sleep5-29517977-xjrnj   0/1     Completed   0          3m5s    10.244.2.60   klc-worker2   <none>           <none>
sleep5-29517978-978mn   0/1     Completed   0          2m5s    10.244.2.64   klc-worker2   <none>           <none>
sleep5-29517978-krp5d   0/1     Completed   0          117s    10.244.2.65   klc-worker2   <none>           <none>
sleep5-29517978-l4fzc   0/1     Completed   0          2m5s    10.244.1.40   klc-worker    <none>           <none>
sleep5-29517978-n7wgb   0/1     Completed   0          2m5s    10.244.2.63   klc-worker2   <none>           <none>
sleep5-29517978-zzq4d   0/1     Completed   0          117s    10.244.1.41   klc-worker    <none>           <none>
sleep5-29517979-28gcj   0/1     Completed   0          57s     10.244.2.68   klc-worker2   <none>           <none>
sleep5-29517979-7r5d5   0/1     Completed   0          65s     10.244.1.42   klc-worker    <none>           <none>
sleep5-29517979-l2mbg   0/1     Completed   0          65s     10.244.2.67   klc-worker2   <none>           <none>
sleep5-29517979-w8mgl   0/1     Completed   0          57s     10.244.1.43   klc-worker    <none>           <none>
sleep5-29517979-wnzpw   0/1     Completed   0          65s     10.244.2.66   klc-worker2   <none>           <none>
sleep5-29517980-5zdxj   1/1     Running     0          5s      10.244.1.44   klc-worker    <none>           <none>
sleep5-29517980-8fxl8   1/1     Running     0          5s      10.244.2.69   klc-worker2   <none>           <none>
sleep5-29517980-nfj5l   1/1     Running     0          5s      10.244.2.70   klc-worker2   <none>           <none>

# get applied yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs/cronjob$ kubectl get job/sleep5-29517994 -o yaml | tee sleep5-applied.yaml

# cronjob
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs/cronjob$ kubectl get cronjob/sleep5

# delete cronjob
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/jobs/cronjob$ kubectl delete cronjob/sleep5

###################
#### configmap ####
###################

# create a config map yaml using cli
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl create configmap colorcmcli --dry-run=client --from-literal=color=red --from-literal=key=value -o yaml | tee colorcmcli.yaml

# get the applied yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl get configmap -o yaml | tee colorcmcli-applied.yaml

# delete the config map
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl delete configmap/colorcmcli

# create a properties file
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ cat <<eof > colorcm.properties
> color=green
> key=value
> eof

# create config file using the properties file
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl create configmap colorcmfile --from-env-file=colorcm.properties

# describe both config maps. their structure is the same.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl describe configmap/colorcmcli
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl describe configmap/colorcmfile

# create a ubuntu pod that dumps all the environment variables
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl run ubuntupod --image=ubuntu --dry-run=client --restart=Never -o yaml --command bash -- -c 'env;sleep infinity' | tee ubuntupod.yaml

# apply the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl apply -f ubuntupod.yaml

# get the logs
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl logs ubuntupod

# output
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=ubuntupod
PWD=/
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
_=/usr/bin/env

# we will edit the pod yaml and load the config map that are applied in the cluster (kubectl get configmap)
# once ubuntupod.yaml is edited to load the configmap, we delete the existing pod, apply ubuntupod yaml, sleep for 5 seconds and get the logs
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl delete pod/ubuntupod --now; kubectl apply -f ubuntupod.yaml; sleep 5; kubectl logs ubuntupod

# output
pod "ubuntupod" deleted from default namespace
pod/ubuntupod created
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=ubuntupod
color=red
PWD=/
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
key=value
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
_=/usr/bin/env

# if we set immutable as true in the config map configuration, then it cannot be edited when it is applied. make colorcmcli.yaml as immutable
# delete colorcmcli and reapply
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl delete configmap/colorcmcli --now; kubectl apply -f colorcmcli.yaml; sleep 5; kubectl get configmap/colorcmcli -o yaml | tee colorcmcli-applied.yaml

# now create ubuntupod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl delete pod/ubuntupod --now; kubectl apply -f ubuntupod.yaml; sleep 5; kubectl logs ubuntupod

# try to edit the config map
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/configmap$ kubectl edit configmap/colorcmcli

# output
error: configmaps "colorcmcli" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-4025637100.yaml"
error: Edit cancelled, no valid changes were saved.

###############
### secrets ###
###############

# create ubuntu pod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ kubectl run ubuntupod --image=ubuntu --dry-run=client --restart=Never -o yaml --command bash -- -c 'env;sleep infinity' | tee ubuntupod.yaml

# get logs
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ kubectl logs ubuntupod

# output
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=ubuntupod
PWD=/
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
_=/usr/bin/env

# create a secret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ kubectl create secret generic colorsecret --from-literal=color=red --from-literal=key=value --dry-run=client -o yaml | tee colorsecret.yaml

# output of this will match the encoded value in colorsecret.yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ echo -n red | base64

# apply the secret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ kubectl apply -f colorsecret.yaml

# edit ubuntupod.yaml to use the secret
# delete, apply and get logs
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets$ kubectl delete pod/ubuntupod --now; kubectl apply -f ubuntupod.yaml ; sleep 5; kubectl logs ubuntupod

# output
pod "ubuntupod" deleted from default namespace
pod/ubuntupod created
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=ubuntupod
color=red
PWD=/
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
key=value
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
_=/usr/bin/env

# create tls key and crt files
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ openssl req -x509 -nodes -newkey rsa:2048 -keyout tls.key -out tls.crt -days 9999 -subj "/C=US/ST=Oklahoma/L=Norman/O=MyCompany/OU=IT/CN=ubuntudev.com"

# create a tls scret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ kubectl create secret tls ubuntudevsecret --cert=tls.crt --key=tls.key --dry-run=client -o yaml | tee ubuntudevsecret.yaml

# apply secret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ kubectl apply -f ubuntudevsecret.yaml

# get json version of applied secret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ kubectl get secret ubuntudevsecret -o json | tee ubuntudevsecret-applied.json


# get data from secret
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ jq -r '.data["tls.crt"]' ubuntudevsecret-applied.json | base64 -d | openssl x509 -noout -subject -issuer -dates

# output
subject=C = US, ST = Oklahoma, L = Norman, O = MyCompany, OU = IT, CN = ubuntudev.com
issuer=C = US, ST = Oklahoma, L = Norman, O = MyCompany, OU = IT, CN = ubuntudev.com
notBefore=Feb 14 17:13:19 2026 GMT
notAfter=Jul  1 17:13:19 2053 GMT

# alternate way to combine generating json, creating file, and loading with jq and getting output
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/secrets/tls$ kubectl get secret ubuntudevsecret -o json > ubuntudevsecret-applied.json; jq -r '.data["tls.crt"]' ubuntudevsecret-applied.json | base64 -d | openssl x509 -noout -subject -issuer -dates

################
#### labels ####
################

# create nginx pod
sysuser@ubuntudev:~$ kubectl run nginxpod --dry-run=client --image=nginx --port=80 -o yaml | tee nginxpod.yaml

# expose the pod
sysuser@ubuntudev:~$ kubectl expose pod/nginxpod --dry-run=client -o yaml | tee nginxcip.yaml

# take note of selector. run the following
sysuser@ubuntudev:~$ kubectl get all -l run=nginxpod

# output
NAME           READY   STATUS    RESTARTS   AGE
pod/nginxpod   1/1     Running   0          6m9s

NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/nginxpod   ClusterIP   10.96.15.106   <none>        80/TCP    4m25s

# create a yaml file with 3 pod configuratios
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ yaml=$(kubectl run ubuntupod --image=ubuntu --dry-run=client -o yaml --command sleep infinity)
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ echo -e "${yaml}\n---\n${yaml}\n---\n${yaml}" | tee ubuntupods.yaml

# manually modify the yaml, change labls to include colors. apply the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ kubectl apply -f ubuntupods.yaml

# output
pod/ubuntupod-red created
pod/ubuntupod-green created
pod/ubuntupod-blue created

# get pods
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ kubectl get pods -o wide

# output
NAME              READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
nginxpod          1/1     Running   0          19m   10.244.2.2   klc-worker2   <none>           <none>
ubuntupod-blue    1/1     Running   0          55s   10.244.1.3   klc-worker    <none>           <none>
ubuntupod-green   1/1     Running   0          55s   10.244.2.3   klc-worker2   <none>           <none>
ubuntupod-red     1/1     Running   0          55s   10.244.1.2   klc-worker    <none>           <none>

# get pods based on labels
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ kubectl get all -l color=red
NAME                READY   STATUS    RESTARTS   AGE
pod/ubuntupod-red   1/1     Running   0          2m6s

# modify yaml, create another pod with name ubuntupod-red2, apply file
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ kubectl apply -f ubuntupods.yaml

# output
pod/ubuntupod-red unchanged
pod/ubuntupod-green unchanged
pod/ubuntupod-blue unchanged
pod/ubuntupod-red2 created

# run selection command
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/labels$ kubectl get all -l color=red

# output
NAME                 READY   STATUS    RESTARTS   AGE
pod/ubuntupod-red    1/1     Running   0          3m16s
pod/ubuntupod-red2   1/1     Running   0          4s

#################
#### storage ####
#################

# create ubuntu pod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl run ubuntupodemptydir --image=ubuntu --dry-run=client -o yaml --command sleep infinity | tee ubuntupodemptydir.yaml

# apply yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl apply -f ubuntupodemptydir.yaml

# shell into the pod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl exec -it ubuntupodemptydir -- bash

# cd into cache directory
root@ubuntupodemptydir:/# cd cache/

# get the file system
root@ubuntupodemptydir:/cache# df -h .

# output
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           1.9G     0  1.9G   0% /cache

# run a test
root@ubuntupodemptydir:/cache# dd if=/dev/zero of=output oflag=sync bs=1024k count=100

# output
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.67878 s, 154 MB/s
root@ubuntupodemptydir:/cache#

# get storage class
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get storageclass

# output
NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
standard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  6h57m

# create a manual persistent volume and apply it
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl apply -f pvmanual.yaml

# get pv
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get pv

# output
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pvmanual   1Gi        RWO            Retain           Available           standard       <unset>                          6s

# create manual persistent volume claim and apply it
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl apply -f pvcmanual.yaml

# get persistent volume claims
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get pvc -o wide

# output
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE   VOLUMEMODE
pvcmanual   Bound    pvmanual   1Gi        RWO            standard       <unset>                 4s    Filesystem

# create dynamic pvc and apply it
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl apply -f pvcdynamic.yaml

# get pvc
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get pvc -o wide

# output
NAME         STATUS    VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE     VOLUMEMODE
pvcdynamic   Pending                                        standard       <unset>                 13s     Filesystem
pvcmanual    Bound     pvmanual   1Gi        RWO            standard       <unset>                 2m49s   Filesystem

# describe pvcdynamic
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl describe pvc/pvcdynamic

# output
Name:          pvcdynamic
Namespace:     default
StorageClass:  standard
Status:        Pending
Volume:
Labels:        <none>
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       <none>
Events:
  Type    Reason                Age                From                         Message
  ----    ------                ----               ----                         -------
  Normal  WaitForFirstConsumer  7s (x62 over 15m)  persistentvolume-controller  waiting for first consumer to be created before binding

# create a new pod yaml and reference the two volume claims in the yaml, and apply it
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl apply -f ubuntupodpv.yaml

# shell into the pod that
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl exec -it ubuntupodpv -- bash

# create two files in dynamic and manual volumes
root@ubuntupodpv:/# echo manual > /manual/manual.txt; echo dynamic > /dynamic/dynamic.txt

# how to figure out where these files are located outside of the container
# what node is the pod running on. in this case it is klc-worker
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get pods -o wide

# output
NAME                READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES
ubuntupodemptydir   1/1     Running   0          101m   10.244.2.5   klc-worker2   <none>           <none>
ubuntupodpv         1/1     Running   0          22m    10.244.1.5   klc-worker    <none>           <none>

# get the path of the volume on the host (klc-worker)
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/storage$ kubectl get pv "$(kubectl get pvc pvcdynamic -o jsonpath='{.spec.volumeName}')" -o jsonpath='{.spec.hostPath.path}'

# output (this is the path of the share on the node - klc-worker)
/var/local-path-provisioner/pvc-ad2e4457-e2e1-424b-b059-a95001a2c26a_default_pvcdynamic

# shell into klc-worker
sysuser@ubuntudev:~$ docker exec -it klc-worker bash

# cd into the directory
root@klc-worker:~# cd /var/local-path-provisioner/pvc-ad2e4457-e2e1-424b-b059-a95001a2c26a_default_pvcdynamic

# list files
root@klc-worker:/var/local-path-provisioner/pvc-ad2e4457-e2e1-424b-b059-a95001a2c26a_default_pvcdynamic# ls -l

# output
-rw-r--r-- 1 root root 8 Feb 15 21:28 dynamic.txt

#####################
#### statefulset ####
#####################

# first create a deployment. easier to edit this than to create a statefulset by hand
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl create deployment nginxsts --image=nginx --dry-run=client -o yaml | tee nginxsts.yaml

# apply the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl apply -f nginxsts.yaml

# get statefulset information
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl get sts -o wide

# output
NAME       READY   AGE   CONTAINERS   IMAGES
nginxsts   5/5     55s   nginx        nginx

# get pods
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl get pods -o wide

NAME         READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
nginxsts-0   1/1     Running   0          90s   10.244.1.2   klc-worker    <none>           <none>
nginxsts-1   1/1     Running   0          81s   10.244.2.2   klc-worker2   <none>           <none>
nginxsts-2   1/1     Running   0          79s   10.244.1.3   klc-worker    <none>           <none>
nginxsts-3   1/1     Running   0          77s   10.244.2.3   klc-worker2   <none>           <none>
nginxsts-4   1/1     Running   0          76s   10.244.1.4   klc-worker    <none>           <none>

# to create the network identity for a statefulset a headsless service (clusterip with no ip) is needed
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl create service clusterip --clusterip=None nginxsts --dry-run=client -o yaml | tee headlesscip.yaml

# apply the headless service
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl apply -f headlesscip.yaml

# get endpoint slices
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl get endpointslice -o wide

# output
NAME             ADDRESSTYPE   PORTS     ENDPOINTS                                      AGE
kubernetes       IPv4          6443      172.18.0.4                                     37h
nginxpod-c547g   IPv4          <unset>   <unset>                                        35h
nginxsts-42hql   IPv4          <unset>   10.244.2.3,10.244.2.2,10.244.1.3 + 2 more...   64s

# get the applied yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl get endpointslices/nginxsts-42hql -o yaml | tee headless-applied.yaml

# create and apply curlpod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl run curlpod --dry-run=client -o yaml --image=curlimages/curl --restart=Never --command -- sh -c "sleep infinity" | tee curlpod.yaml; kubectl apply -f curlpod.yaml

# shell into curlpod
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl exec -it curlpod -- sh

# curl one of the hosts using dns entry
~ $ curl nginxsts-0.nginxsts.default.svc.cluster.local

# get applied yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl get statefulset -o yaml | tee nginxsts-applied.yaml

# change the partition to 1 and watch the rolling updates. edit the applied yaml file and apply it.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl apply -f nginxsts-applied.yaml

# output
statefulset.apps/nginxsts configured

# change the image and watch the rollout
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/statefulset$ kubectl set image statefulset/nginxsts nginx=nginx:alpine && kubectl rollout status statefulset/nginxsts

# output
statefulset.apps/nginxsts image updated
Waiting for partitioned roll out to finish: 0 out of 4 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 1 out of 4 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 2 out of 4 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 3 out of 4 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
partitioned roll out complete: 4 new pods have been updated...

# edit the stateful set original yaml to include persistent volume information
# exec into one of the pods
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl exec -it nginxsts-0 -- bash

# cd into the data directory and create a file with the pod name
root@nginxsts-0:/# cd /data; touch nginxsts-0.txt

# exit and delete the pod. the pod will
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl delete pod nginxsts-0 --now; watch kubectl get pods -o wide

# now exec back into the pod and ensure that the data persists
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl exec -it nginxsts-0 -- bash

## deleting the entire statefulset will have no impact on the pvc and pv. the data will still live on the nodes.

#########################
#### NetworkPolicies ####
#########################

# create an nginx pod, and save the yaml
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl run nginxpod --image=nginx --dry-run=client -o yaml | tee nginxpod.yaml

# apply the yaml and expose the pod as a cluster ip and write a yaml file
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl apply -f nginxpod.yaml; kubectl expose pod/nginxpod --port=80 --name=nginxcip --dry-run=client -o yaml | tee nginxcip.yaml

# apply the yaml file and get the services
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl apply -f nginxcip.yaml; kubectl get svc -o wide

# create a curl pod and shell into it
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl run -it --rm curlpod --image=curlimages/curl --restart=Never -- sh

# from inside curlpod, curl the clusterip service
~ $ curl nginxcip.default.svc.cluster.local

# create a network policy and apply it.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl apply -f nginxnp.yaml

# get all the network policies
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl get networkpolicy

# create and shell into a curl pod and curl the dns entry for the cip
~ $ curl nginxcip.default.svc.cluster.local

# now create another curl pod called curlpod2 and try the same
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/networkpolicies$ kubectl run -it --rm curlpod2 --image=curlimages/curl --restart=Never -- sh

# curl the dns entry for the cluster ip. this will not successful.
~ $ curl nginxcip.default.svc.cluster.local

# output
curl: (28) Failed to connect to nginxcip.default.svc.cluster.local port 80 after 136464 ms: Could not connect to server

#############################
#### PodDisruptionBudget ####
#############################

# create a deployment of nginx with 5 replicas.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl create deployment nginxdep --image=nginx --replicas=5 --dry-run=client -o yaml | tee nginxdep.yaml

# apply the yaml and get the pod status
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl apply -f nginxdep.yaml; kubectl get pods -o wide

# output
NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
nginxdep-5f4d9669f8-gxv8d   1/1     Running   0          28s   10.244.2.9    klc-worker    <none>           <none>
nginxdep-5f4d9669f8-jrx8d   1/1     Running   0          28s   10.244.1.6    klc-worker2   <none>           <none>
nginxdep-5f4d9669f8-nphvd   1/1     Running   0          28s   10.244.1.7    klc-worker2   <none>           <none>
nginxdep-5f4d9669f8-nzt7z   1/1     Running   0          28s   10.244.2.11   klc-worker    <none>           <none>
nginxdep-5f4d9669f8-w7b9j   1/1     Running   0          28s   10.244.2.10   klc-worker    <none>           <none>

# use cordon command to make one node not seheduleable
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl cordon klc-worker

# get all the pods belonging to a specific node (klc-worker) and so all pods will now be running on klc-worker2
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl get pods --field-selector spec.nodeName=klc-worker1 -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | xargs -r -n1 kubectl delete pod

# now drain klc-worker2
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl drain klc-worker2 --delete-emptydir-data=true --ignore-daemonsets=true

# output
node/klc-worker2 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/kindnet-w9s4b, kube-system/kube-proxy-cnljk
evicting pod default/nginxdep-5f4d9669f8-z8dw7
evicting pod default/nginxdep-5f4d9669f8-68z45
evicting pod default/nginxdep-5f4d9669f8-7n7bp
evicting pod default/nginxdep-5f4d9669f8-brv4h
evicting pod default/nginxdep-5f4d9669f8-h9t87
pod/nginxdep-5f4d9669f8-z8dw7 evicted
pod/nginxdep-5f4d9669f8-68z45 evicted
pod/nginxdep-5f4d9669f8-h9t87 evicted
pod/nginxdep-5f4d9669f8-7n7bp evicted
pod/nginxdep-5f4d9669f8-brv4h evicted
node/klc-worker2 drained

# get the deployment and pod status
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl get deployment -o wide; kubectl get pods -o wide

# output
NAME       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
nginxdep   0/5     5            0           32m   nginx        nginx    app=nginxdep
NAME                        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginxdep-5f4d9669f8-4627f   0/1     Pending   0          54s   <none>   <none>   <none>           <none>
nginxdep-5f4d9669f8-4h2l2   0/1     Pending   0          54s   <none>   <none>   <none>           <none>
nginxdep-5f4d9669f8-56jv4   0/1     Pending   0          54s   <none>   <none>   <none>           <none>
nginxdep-5f4d9669f8-7bgjz   0/1     Pending   0          54s   <none>   <none>   <none>           <none>
nginxdep-5f4d9669f8-vm9xk   0/1     Pending   0          54s   <none>   <none>   <none>           <none>

# in a new terminal window watch the pod status. these pods will transition from pending once the nodes are uncordoned.
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ watch kubectl get pods -o wide

# in the previous terminal window, uncordon the nodes
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl uncordon klc-worker klc-worker2

# create a pod disruption budget and apply the file
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl create pdb nginxpdb --selector=app=nginxdep --min-available=2 --dry-run=client -o yaml | tee nginxpdb.yaml; kubectl apply -f nginxpdb.yaml

# cordon and drain the worker nodes
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl cordon klc-worker klc-worker2; kubectl drain klc-worker klc-worker2 --delete-emptydir-data=true --ignore-daemonsets=true

# output
node/klc-worker cordoned
node/klc-worker2 cordoned
node/klc-worker already cordoned
node/klc-worker2 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/kindnet-ph67s, kube-system/kube-proxy-cg4gw
evicting pod default/nginxdep-5f4d9669f8-vm9xk
evicting pod default/nginxdep-5f4d9669f8-4627f
evicting pod default/nginxdep-5f4d9669f8-56jv4
evicting pod default/nginxdep-5f4d9669f8-7bgjz
evicting pod default/nginxdep-5f4d9669f8-4h2l2
error when evicting pods/"nginxdep-5f4d9669f8-7bgjz" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
error when evicting pods/"nginxdep-5f4d9669f8-56jv4" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
pod/nginxdep-5f4d9669f8-vm9xk evicted
pod/nginxdep-5f4d9669f8-4627f evicted
pod/nginxdep-5f4d9669f8-4h2l2 evicted
evicting pod default/nginxdep-5f4d9669f8-56jv4
evicting pod default/nginxdep-5f4d9669f8-7bgjz
error when evicting pods/"nginxdep-5f4d9669f8-56jv4" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
error when evicting pods/"nginxdep-5f4d9669f8-7bgjz" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.

# uncordon one of the workers
sysuser@ubuntudev:~/git/ldk/kcna/kubernetes/pdb$ kubectl uncordon klc-worker2

##################
#### security ####
##################

# create a custom rootshell ubuntu pod
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl run ubrootshell --image=spurin/rootshell --dry-run=client -o yaml -- sleep infinity | tee ubrootshell.yaml

# apply the yaml
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl apply -f ubrootshell.yaml

# shell into the pod
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl exec -it ubrootshell -- bash

# check user id
root@ubrootshell:/# id

# output
uid=0(root) gid=0(root) groups=0(root)
root@ubrootshell:/# exit
exit

# update the yaml to include security policy, and force re apply
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl replace --force -f ubrootshell.yaml

# shell into the pod
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl exec -it ubrootshell -- bash

# check user id (the prompt indicates that we are not root)
nonpriv@ubrootshell:/$ id

# output
uid=1000(nonpriv) gid=1000(nonpriv) groups=1000(nonpriv)

# run the rootshell binary to convert to root
nonpriv@ubrootshell:/$ /rootshell

# output
nonpriv@ubrootshell:/$ /rootshell
root@ubrootshell:/# id
uid=0(root) gid=1000(nonpriv) groups=1000(nonpriv)
root@ubrootshell:/#

# update the yaml to prevent privilege escalation via securityContext and recreate the pod
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl delete pod ubrootshell --now;kubectl apply -f ubrootshell.yaml

# shell into the pod
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/security$ kubectl exec -it ubrootshell -- bash

# run the rootshell binary to convert to root. this will not work. type exit twice to exit out of pod shell
nonpriv@ubrootshell:/$ /rootshell

##############
#### helm ####
##############

# install helm and run the following
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/helm$ helm create flappydock

# this will create a service that is serving on container port 80. port forward this to 8080 on localhost
sysuser@ubuntuprod:~/git/ldk/kcna/kubernetes/helm/flappydock$ kubectl -n default port-forward svc/flappydock 1234:$(kubectl get pod -n default $(kubectl get pods -n default -l 'app.kubernetes.io/name=flappydock,app.kubernetes.io/instance=flappydock' -o jsonpath='{.items[0].metadata.name}') -o jsonpath='{.spec.containers[0].ports[0].containerPort}')

